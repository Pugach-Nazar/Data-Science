{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab11\n",
    "\n",
    "Вступ до Natural Language Processing (NLP)\n",
    "\n",
    "Мета: Познайомитися з основними поняттями, методами та підходами у сфері обробки природної мови (NLP). Провести порівняльний аналіз популярних алгоритмів та інструментів, а також підготувати презентацію на цю тему.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Теоретичне дослідження:\n",
    "- Дослідіть основні етапи NLP, такі як:\n",
    "    - Токенізація — це процес розбиття тексту на окремі одиниці (токени), такі як слова, фрази чи символи. Це початковий етап, який дозволяє системі обробляти текст як набір окремих елементів. Існують два основних типи токенізації:\n",
    "        - Словесна токенізація: розбиття на окремі слова.\n",
    "        - Токенізація на рівні символів: розбиття на окремі символи або літери.\n",
    "    - Лемматизація та стемінг\n",
    "        - Стемінг: це процес відсічення суфіксів та префіксів зі слів для отримання їхнього кореня. Наприклад, слова \"running\", \"runner\" можуть бути приведені до \"run\". Стемінг може бути агресивним і не завжди надає лексичну основу слова.\n",
    "        - Лемматизація: це більш складний і точний процес, який зберігає значення слова. Лемматизатор обчислює граматичну форму слова (наприклад, \"better\" -> \"good\").\n",
    "    - Векторизація тексту (Bag of Words, TF-IDF, Word Embeddings) це процес перетворення текстових даних у числову форму, яка дозволяє використовувати їх для машинного навчання. Існують кілька методів векторизації:\n",
    "        - Bag of Words (BoW): кожне слово в тексті стає окремим елементом у векторі, а значення елементів є кількістю появ кожного слова в тексті\n",
    "        - TF-IDF (Term Frequency-Inverse Document Frequency) — це метод, що оцінює важливість слова в контексті всього корпусу тексту. Він зменшує вагу часто вживаних слів (наприклад, \"the\", \"is\") і надає більшу вагу рідше вживаним\n",
    "        - Word Embeddings: це методи, які створюють вектори для слів, де слова з подібними значеннями знаходяться ближче один до одного в багатовимірному просторі. Найвідоміші моделі — Word2Vec, GloVe, FastText\n",
    "    - Класифікація тексту — це процес віднесення тексту до певної категорії або класу. Це один із основних застосунків NLP, наприклад, для автоматичного маркування електронних листів як спаму чи не спаму, або визначення тематики статей.\n",
    "    - Розпізнавання сутностей (Named Entity Recognition, NER) — це процес виявлення та класифікації сутностей у тексті. Це можуть бути: імена людей, локації, дати, компанії (наприклад AWS) тощо.\n",
    "- Вивчіть ключові моделі для NLP, такі як наївний баєсовий класифікатор, логістична регресія, LSTM, Transformers, та GPT.\n",
    "    - Наївний баєсовий класифікатор:\n",
    "        - Класифікація тексту\n",
    "        - Швидкість, простота, ефективність на великих даних\n",
    "        - Не підходить для складних залежностей\n",
    "    - Логістична регресія\n",
    "        - Бінарна і багатокласова класифікація\n",
    "        - Простота, швидкість, інтерпретованість\n",
    "        - Не обробляє складні залежності\n",
    "    - LSTM (Long Short-Term Memory)\n",
    "        - Обробка послідовностей (переклад, генерація тексту)\n",
    "        - Обробка довгих залежностей, контексту\n",
    "        - Велика обчислювальна вартість\n",
    "    - Transformers \n",
    "        - Машинний переклад, генерація тексту\n",
    "        - Паралельне навчання, обробка довгих текстів\n",
    "        - Великі обчислювальні ресурси\n",
    "    - GPT (Generative Pre-trained Transformer)\n",
    "        - Генерація тексту, питання/відповіді\n",
    "        - Висока якість генерації тексту\n",
    "        - Ресурсомісткість, можливість помилок\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Порівняльний аналіз методів векторизації тексту:\n",
    "- Створіть таблицю, яка порівнює різні підходи до векторизації тексту:\n",
    "    - Bag of Words (BOW)\n",
    "    - TF-IDF\n",
    "    - Word Embeddings (Word2Vec, GloVe)\n",
    "- У таблиці зазначте основні характеристики кожного методу, такі як:\n",
    "    - Переваги та недоліки\n",
    "    - Складність реалізації\n",
    "    - Застосування в різних задачах\n",
    "    - Складність обробки великих обсягів даних\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Метод                | Переваги                                     | Недоліки                                   | Складність реалізації | Застосування в різних задачах                    | Складність обробки великих обсягів даних |\n",
    "|----------------------|----------------------------------------------|--------------------------------------------|------------------------|------------------------------------------------|------------------------------------------|\n",
    "| **Bag of Words (BOW)** | Проста реалізація, швидка обробка для невеликих даних | Ігнорує порядок слів, високий вимір для великих словників | Легка                   | Класифікація тексту, аналіз настроїв, спам-фільтрація | Висока, особливо для великих наборів текстів |\n",
    "| **TF-IDF**            | Враховує важливість слів в контексті всього набору текстів, знижує вагу поширених слів | Не враховує контекст і семантику, може бути неефективним для дуже великих корпусів | Легка                   | Класифікація тексту, пошук інформації, аналіз тем | Середня (потребує обчислення IDF для всього корпусу) |\n",
    "| **Word Embeddings (Word2Vec, GloVe)** | Враховує контекст слів, слова з подібним значенням знаходяться ближче один до одного | Складність тренування, може потребувати великого обсягу даних | Складна                 | Генерація тексту, машинний переклад, аналіз семантики | Низька для GloVe, висока для Word2Vec через тренування |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Огляд інструментів для NLP:\n",
    "- Підготуйте таблицю порівняння інструментів та бібліотек, що часто використовуються для NLP, таких як:\n",
    "    - NLTK\n",
    "    - SpaCy\n",
    "    - Hugging Face Transformers\n",
    "    - Gensim\n",
    "- Вкажіть основні функції, підтримку різних мов, простоту використання, та особливості кожного інструмента.\n",
    "- Приклади можливих застосувань NLP в різних галузях (наприклад, аналіз тональності, чат-боти, рекомендаційні системи).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Інструмент/Biblioteka        | Основні функції                                      | Підтримка мов       | Простота використання | Особливості                               | Приклади застосувань                                         |\n",
    "|-----------------------------|-----------------------------------------------------|---------------------|------------------------|-------------------------------------------|------------------------------------------------------------|\n",
    "| **NLTK (Natural Language Toolkit)** | Токенізація, стемінг, лемматизація, POS tagging, Named Entity Recognition (NER), синтаксичний аналіз | Підтримує англійську та інші поширені мови | Помірно складний | Великий набір інструментів, добре документований | Аналіз тональності, тематичне моделювання, чат-боти       |\n",
    "| **SpaCy**                    | Токенізація, POS tagging, NER, синтаксичний аналіз, векторизація слів, залежності | Підтримка понад 60 мов, зокрема англійська, німецька, французька | Легкий у використанні | Дуже швидкий, орієнтований на продуктивність, готові моделі для багатьох мов | Чат-боти, аналіз тексту, автоматичне резюмування         |\n",
    "| **Hugging Face Transformers** | Трансформери для моделювання тексту (BERT, GPT, T5, RoBERTa, etc.), генерація тексту, питання-відповідь | Підтримка всіх основних мов, зокрема англійської, китайської, арабської | Легкий у використанні, але вимагає знань глибокого навчання | Інтерфейс високого рівня для трансформерів, інтеграція з PyTorch та TensorFlow | Генерація тексту, переклад, аналіз тональності, чат-боти |\n",
    "| **Gensim**                   | Тематичне моделювання (LDA), Word2Vec, FastText, Doc2Vec, векторизація тексту | Підтримка багатьох мов | Простий для використання | Орієнтований на векторизацію та тематичне моделювання, оптимізований для великих обсягів тексту | Рекомендаційні системи, тематичне моделювання, пошук схожих документів |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Висновки:\n",
    "- Для базових задач NLP, таких як токенізація, стемінг і лемматизація, добре підходять NLTK та SpaCy. SpaCy є більш швидким і оптимізованим для великих обсягів даних, тому для більш складних задач краще вибирати SpaCy.\n",
    "- Для задач, пов'язаних із генерацією тексту, перекладом або аналізом контексту, найкраще використовувати Hugging Face Transformers, оскільки трансформери забезпечують найкращі результати в цих сферах.\n",
    "- Для тематичного моделювання та векторизації тексту на великих даних найкращим вибором є Gensim. Він також підходить для рекомендаційних систем.\n",
    "BOW та TF-IDF підходять для простих задач, таких як класифікація текстів або пошук інформації, але мають обмеження щодо контексту та порядку слів."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
